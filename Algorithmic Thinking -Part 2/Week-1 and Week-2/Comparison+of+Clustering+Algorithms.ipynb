{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of two clustering Algorithms\n",
    "\n",
    "1. Hierarchical Clustering\n",
    "2. K-Means Clustering\n",
    "\n",
    "we will compare these two clustering methods in three areas:<br>\n",
    "\n",
    "<b>Efficiency</b> - Which method computes clusterings more efficiently?<br>\n",
    "<b>Automation</b> - Which method requires less human supervision to generate reasonable clusterings?<br>\n",
    "<b>Quality</b> - Which method generates clusterings with less error?<br>\n",
    "\n",
    "<br>\n",
    "We will apply our clustering methods to several sets of 2D data that include information about lifetime cancer risk from air toxics. The raw version of this data is available from <a href=\"https://www.epa.gov/national-air-toxics-assessment/2005-national-air-toxics-assessment\">this</a> website.<br> \n",
    "Each entry in the data set corresponds to a county in the USA (identified by a unique 5 digit string called a FIPS county code) and includes information on the total population of the county and its per-capita lifetime cancer risk due to air toxics.\n",
    "\n",
    "<br>\n",
    "To visualizing this data, we have processed this county-level data to include the (x,y) position of each county when overlaid on <a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/USA_Counties_with_FIPS_and_names.svg/1000px-USA_Counties_with_FIPS_and_names.svg.png\">this</a> map of the USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cluster class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster class for Module 3\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    \"\"\"\n",
    "    Class for creating and merging clusters of counties\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fips_codes, horiz_pos, vert_pos, population, risk):\n",
    "        \"\"\"\n",
    "        Create a cluster based the models a set of counties' data\n",
    "        \"\"\"\n",
    "        self._fips_codes = fips_codes\n",
    "        self._horiz_center = horiz_pos\n",
    "        self._vert_center = vert_pos\n",
    "        self._total_population = population\n",
    "        self._averaged_risk = risk\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        String representation assuming the module is \"alg_cluster\".\n",
    "        \"\"\"\n",
    "        rep = \"alg_cluster.Cluster(\"\n",
    "        rep += str(self._fips_codes) + \", \"\n",
    "        rep += str(self._horiz_center) + \", \"\n",
    "        rep += str(self._vert_center) + \", \"\n",
    "        rep += str(self._total_population) + \", \"\n",
    "        rep += str(self._averaged_risk) + \")\"\n",
    "        return rep\n",
    "\n",
    "\n",
    "    def fips_codes(self):\n",
    "        \"\"\"\n",
    "        Get the cluster's set of FIPS codes\n",
    "        \"\"\"\n",
    "        return self._fips_codes\n",
    "    \n",
    "    def horiz_center(self):\n",
    "        \"\"\"\n",
    "        Get the averged horizontal center of cluster\n",
    "        \"\"\"\n",
    "        return self._horiz_center\n",
    "    \n",
    "    def vert_center(self):\n",
    "        \"\"\"\n",
    "        Get the averaged vertical center of the cluster\n",
    "        \"\"\"\n",
    "        return self._vert_center\n",
    "    \n",
    "    def total_population(self):\n",
    "        \"\"\"\n",
    "        Get the total population for the cluster\n",
    "        \"\"\"\n",
    "        return self._total_population\n",
    "    \n",
    "    def averaged_risk(self):\n",
    "        \"\"\"\n",
    "        Get the averaged risk for the cluster\n",
    "        \"\"\"\n",
    "        return self._averaged_risk\n",
    "   \n",
    "        \n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        Return a copy of a cluster\n",
    "        \"\"\"\n",
    "        copy_cluster = Cluster(set(self._fips_codes), self._horiz_center, self._vert_center,\n",
    "                               self._total_population, self._averaged_risk)\n",
    "        return copy_cluster\n",
    "\n",
    "\n",
    "    def distance(self, other_cluster):\n",
    "        \"\"\"\n",
    "        Compute the Euclidean distance between two clusters\n",
    "        \"\"\"\n",
    "        vert_dist = self._vert_center - other_cluster.vert_center()\n",
    "        horiz_dist = self._horiz_center - other_cluster.horiz_center()\n",
    "        return math.sqrt(vert_dist ** 2 + horiz_dist ** 2)\n",
    "        \n",
    "    def merge_clusters(self, other_cluster):\n",
    "        \"\"\"\n",
    "        Merge one cluster into another\n",
    "        The merge uses the relatively populations of each\n",
    "        cluster in computing a new center and risk\n",
    "        \n",
    "        Note that this method mutates self\n",
    "        \"\"\"\n",
    "        if len(other_cluster.fips_codes()) == 0:\n",
    "            return self\n",
    "        else:\n",
    "            self._fips_codes.update(set(other_cluster.fips_codes()))\n",
    " \n",
    "            # compute weights for averaging\n",
    "            self_weight = float(self._total_population)                        \n",
    "            other_weight = float(other_cluster.total_population())\n",
    "            self._total_population = self._total_population + other_cluster.total_population()\n",
    "            self_weight /= self._total_population\n",
    "            other_weight /= self._total_population\n",
    "                    \n",
    "            # update center and risk using weights\n",
    "            self._vert_center = self_weight * self._vert_center + other_weight * other_cluster.vert_center()\n",
    "            self._horiz_center = self_weight * self._horiz_center + other_weight * other_cluster.horiz_center()\n",
    "            self._averaged_risk = self_weight * self._averaged_risk + other_weight * other_cluster.averaged_risk()\n",
    "            return self\n",
    "\n",
    "    def cluster_error(self, data_table):\n",
    "        \"\"\"\n",
    "        Input: data_table is the original table of cancer data used in creating the cluster.\n",
    "        \n",
    "        Output: The error as the sum of the square of the distance from each county\n",
    "        in the cluster to the cluster center (weighted by its population)\n",
    "        \"\"\"\n",
    "        # Build hash table to accelerate error computation\n",
    "        fips_to_line = {}\n",
    "        for line_idx in range(len(data_table)):\n",
    "            line = data_table[line_idx]\n",
    "            fips_to_line[line[0]] = line_idx\n",
    "        \n",
    "        # compute error as weighted squared distance from counties to cluster center\n",
    "        total_error = 0\n",
    "        counties = self.fips_codes()\n",
    "        for county in counties:\n",
    "            line = data_table[fips_to_line[county]]\n",
    "            singleton_cluster = Cluster(set([line[0]]), line[1], line[2], line[3], line[4])\n",
    "            singleton_distance = self.distance(singleton_cluster)\n",
    "            total_error += (singleton_distance ** 2) * singleton_cluster.total_population()\n",
    "        return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods for clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "slow_closest_pair(cluster_list)\n",
    "fast_closest_pair(cluster_list)\n",
    "closest_pair_strip(cluster_list, horiz_center, half_width)\n",
    "\n",
    "where cluster_list is a 2D list of clusters in the plane\n",
    "\"\"\"\n",
    "\n",
    "######################################################\n",
    "# Code for closest pairs of clusters\n",
    "\n",
    "def pair_distance(cluster_list, idx1, idx2):\n",
    "    \"\"\"\n",
    "    Helper function that computes Euclidean distance between two clusters in a list\n",
    "\n",
    "    Input: cluster_list is list of clusters, idx1 and idx2 are integer indices for two clusters\n",
    "    \n",
    "    Output: tuple (dist, idx1, idx2) where dist is distance between\n",
    "    cluster_list[idx1] and cluster_list[idx2]\n",
    "    \"\"\"\n",
    "    return (cluster_list[idx1].distance(cluster_list[idx2]), min(idx1, idx2), max(idx1, idx2))\n",
    "\n",
    "\n",
    "def slow_closest_pair(cluster_list):\n",
    "    \"\"\"\n",
    "    Compute the distance between the closest pair of clusters in a list (slow)\n",
    "\n",
    "    Input: cluster_list is the list of clusters\n",
    "    \n",
    "    Output: tuple of the form (dist, idx1, idx2) where the centers of the clusters\n",
    "    cluster_list[idx1] and cluster_list[idx2] have minimum distance dist.       \n",
    "    \"\"\"\n",
    "    ans = (float(\"inf\"),-1,-1)\n",
    "    for ind_i,point_i in enumerate(cluster_list):\n",
    "        for ind_j,point_j in enumerate(cluster_list):\n",
    "            if ind_i != ind_j :\n",
    "                distance = point_i.distance(point_j)\n",
    "                if distance < ans[0]:\n",
    "                    ans = (distance,ind_i,ind_j)\n",
    "    return ans\n",
    "\n",
    "\n",
    "\n",
    "def fast_closest_pair(cluster_list):\n",
    "    \"\"\"\n",
    "    Compute the distance between the closest pair of clusters in a list (fast)\n",
    "\n",
    "    Input: cluster_list is list of clusters SORTED such that horizontal positions of their\n",
    "    centers are in ascending order\n",
    "    \n",
    "    Output: tuple of the form (dist, idx1, idx2) where the centers of the clusters\n",
    "    cluster_list[idx1] and cluster_list[idx2] have minimum distance dist.       \n",
    "    \"\"\"\n",
    "    num = len(cluster_list)\n",
    "    if num <= 3:\n",
    "        ans = slow_closest_pair(cluster_list)\n",
    "    else:\n",
    "        middle = num // 2\n",
    "        p_left = cluster_list[:middle]\n",
    "        p_right = cluster_list[middle:]\n",
    "        d_left = fast_closest_pair(p_left)\n",
    "        d_right = fast_closest_pair(p_right)\n",
    "        ans = d_left\n",
    "        if d_left[0] > d_right[0]:\n",
    "            ans = (d_right[0],d_right[1]+middle,d_right[2]+middle)\n",
    "        \n",
    "        mid = (cluster_list[middle-1].horiz_center() + cluster_list[middle].horiz_center()) / 2\n",
    "        \n",
    "        ans_2 = closest_pair_strip(cluster_list,mid,ans[0])\n",
    "        if ans_2[0] < ans[0]:\n",
    "            ans = ans_2\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def closest_pair_strip(cluster_list, horiz_center, half_width):\n",
    "    \"\"\"\n",
    "    Helper function to compute the closest pair of clusters in a vertical strip\n",
    "    \n",
    "    Input: cluster_list is a list of clusters produced by fast_closest_pair\n",
    "    horiz_center is the horizontal position of the strip's vertical center line\n",
    "    half_width is the half the width of the strip (i.e; the maximum horizontal distance\n",
    "    that a cluster can lie from the center line)\n",
    "\n",
    "    Output: tuple of the form (dist, idx1, idx2) where the centers of the clusters\n",
    "    cluster_list[idx1] and cluster_list[idx2] lie in the strip and have minimum distance dist.       \n",
    "    \"\"\"\n",
    "    #cluster_list.sort(key = lambda cluster: cluster.vert_center())\n",
    "    index_set = list()\n",
    "    for ind, point in enumerate(cluster_list):\n",
    "        if abs(point.horiz_center()-horiz_center) < half_width :\n",
    "            index_set.append(ind)\n",
    "    index_set = sorted(index_set, key = lambda x: cluster_list[x].vert_center(), reverse = True)\n",
    "    \n",
    "    k_items = len(index_set)\n",
    "    ans = (float(\"inf\"),-1,-1)\n",
    "    for u_ind in range(0,k_items-1):\n",
    "        for v_ind in range(u_ind+1,min(u_ind+3,k_items-1)+1):\n",
    "            distance = cluster_list[index_set[u_ind]].distance(cluster_list[index_set[v_ind]])\n",
    "            if ans[0] > distance :\n",
    "                ans = (distance,min(index_set[u_ind],index_set[v_ind]),max(index_set[u_ind],index_set[v_ind]))\n",
    "    \n",
    "    return ans\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hierarchical_clustering(cluster_list, num_clusters)\n",
    "kmeans_clustering(cluster_list, num_clusters, num_iterations)\n",
    "\"\"\"\n",
    "\n",
    "######################################################################\n",
    "# Code for hierarchical clustering\n",
    "\n",
    "\n",
    "def hierarchical_clustering(cluster_list, num_clusters):\n",
    "    \"\"\"\n",
    "    Compute a hierarchical clustering of a set of clusters\n",
    "    Note: the function may mutate cluster_list\n",
    "    \n",
    "    Input: List of clusters, integer number of clusters\n",
    "    Output: List of clusters whose length is num_clusters\n",
    "    \"\"\"\n",
    "    while len(cluster_list) > num_clusters :\n",
    "        # sort list very importtant\n",
    "        cluster_list = sorted(cluster_list, key = lambda cluster:cluster.horiz_center())\n",
    "        pair = fast_closest_pair((cluster_list))\n",
    "        cluster_list[pair[1]].merge_clusters(cluster_list[pair[2]])\n",
    "        cluster_list.pop(pair[2])\n",
    "\n",
    "    return cluster_list\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Code for k-means clustering\n",
    "\n",
    "    \n",
    "def kmeans_clustering(cluster_list, num_clusters, num_iterations):\n",
    "    \"\"\"\n",
    "    Compute the k-means clustering of a set of clusters\n",
    "    Note: the function may not mutate cluster_list\n",
    "    \n",
    "    Input: List of clusters, integers number of clusters and number of iterations\n",
    "    Output: List of clusters whose length is num_clusters\n",
    "    \"\"\"\n",
    "\n",
    "    # position initial clusters at the location of clusters with largest populations\n",
    "    cluster_list_sorted = sorted(cluster_list, key = lambda cluster:cluster.total_population(), reverse = True)\n",
    "    k_cluster_center = []\n",
    "    for idx in range(0,num_clusters):\n",
    "        k_cluster_center.append((cluster_list_sorted[idx].horiz_center(),cluster_list_sorted[idx].vert_center()))\n",
    "    \n",
    "    for dummy_iter in range(1,num_iterations+1):\n",
    "        #Initialize k empty sets C1, . . . , Ck;\n",
    "        new_cluster = [alg_cluster.Cluster(set([]),0,0,1,0) for dummy_idx in range(0,num_clusters)]\n",
    "        for index in range(0,len(cluster_list)):\n",
    "            distance_list = [math.sqrt((k_cluster_center[ind_f][0] - cluster_list[index].horiz_center()) ** 2 + (k_cluster_center[ind_f][1] - cluster_list[index].vert_center()) **2 ) for ind_f in range(0,num_clusters)]\n",
    "            index_min = distance_list.index(min(distance_list))\t\n",
    "            new_cluster[index_min].merge_clusters(cluster_list[index])\n",
    "            \n",
    "        for index in range(0, num_clusters):\n",
    "            k_cluster_center[index] = (new_cluster[index].horiz_center(),new_cluster[index].vert_center())\n",
    "    \n",
    "    return new_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data to program \n",
    "def load_data_table(data_url):\n",
    "    \"\"\"\n",
    "    Import a table of county-based cancer risk data\n",
    "    from a csv format file\n",
    "    \"\"\"\n",
    "    data_file = urllib2.urlopen(data_url)\n",
    "    data = data_file.read()\n",
    "    data_lines = data.split('\\n')\n",
    "    print(\"Loaded\", len(data_lines), \"data points\")\n",
    "    data_tokens = [line.split(',') for line in data_lines]\n",
    "    return [[tokens[0], float(tokens[1]), float(tokens[2]), int(tokens[3]), float(tokens[4])] \n",
    "            for tokens in data_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency Comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Comparision between running time of slow_closest_pair and fast_closest_pair.<br>\n",
    "\n",
    "  Complexity of fast_closest_pair : O(n log<sup>2</sup> n)  <br> \n",
    "  Complexity of slow_closest_pair : O(n<sup>2</sup>) <br> \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/Running_time_slow_and_fast_closest_pair.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. An image of the 15 clusters generated by applying hierarchical clustering to the 3108 county cancer risk data set.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/Hierachical_clustering_15_cluster_3108.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.An image of the 15 clusters generated by applying 5 iterations of k-means clustering to the 3108 county cancer risk data set.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/K-means_clustering_15_cluster_3108.png.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Which clustering method is faster when the number of output clusters is either a small fixed number or a small fraction of the number of input clusters?<br>\n",
    "Ans : If there are n input clusters and k output clusters, hierarchical clustering makes (n−k) calls to fast_closest_pair. If k is fixed or a small fraction of n, each call to fast_closest_pair is O(nlog<sup>2</sup>⁡n) and, therefore, the running time for hierarchical clustering using fast_closest_pair is O(n<sup>2</sup>log<sup>2</sup>⁡n). <br>\n",
    "\n",
    "For k-means, the running time is either O(n) or O(n<sup>2</sup>) depending on whether the size of output cluster is fixed or varies as a function n. \n",
    "Even in the this second case, k being a small fraction of n will reduce the running time in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation Comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. An image of the 9 clusters generated by applying hierarchical clustering to the 111 county cancer risk data set. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/Hierachical_clustering_9_cluster_111.png.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. An image of the 9 clusters generated by applying k-means clustering to the 111 county cancer risk data set. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/K-means_clustering_9_cluster_111.png.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Write a function compute_distortion(cluster_list) that takes a list of clusters and uses cluster_error to compute its distortion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_table = load_data_table(DATA_XXXXX_URL)\n",
    "\n",
    "def error_count(cluster_list):\n",
    "    error_sum = 0.0\n",
    "    for cluster in cluster_list:\n",
    "        error_sum += cluster.cluster_error(data_table)   ## Cluster class error function\n",
    "    return error_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The distortion(or error) for 9 clusters by hierarchical clustering on 111 county data set is = 1.7516 * 10^11 OR 175163886916.0 <br>\n",
    "2. The distortion for 9 clusters by K-means clustering on 111 county data set is = 2.712 * 10^11 OR 271254226925.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Describe the difference between the shapes of the clusters produced by these two methods on the <a href=\"https://en.wikipedia.org/wiki/West_Coast_of_the_United_States\">west coast of the USA</a>. What caused one method to produce a clustering with a much higher distortion? To help you answer this question, you should consider how k-means clustering generates its initial clustering in this case.\n",
    "\n",
    "Ans : Each method generates 3 clusters on the west coast of the USA. Hierarchical clustering generates one cluster in Washington state, one in northern California and one in southern California. K-means clustering generates one cluster that includes Washington state and parts of northern California, one cluster that includes the Los Angeles area, and one cluster that includes San Diego. The k-means clustering has substantially higher distortion due in part to the fact that southern California is split into two clusters while northern California is clustered with Washington state.<br>\n",
    "<br>\n",
    "This difference in cluster shape is due to the fact that the initial clustering used in k-means clustering includes the 3 counties in southern California with high population and no counties in northern California or Washington state. Due to a poor choice of the initial clustering based on large population counties, k-means clustering produces a clustering with relatively high distortion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Which method (hierarchical clustering or k-means clustering) requires less human supervision to produce clustering with relatively low distortion?<br><br>\n",
    "Ans : Hierarchical clustering requires less human supervision than k-means clustering to produce clustering of relatively low distortion as it requires no human interaction beyond the choice of the number of output clusters. On the other hand, k-means clustering requires a good strategy for choosing the initial cluster centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Comparison between Distortion of hierarchical and k-means methods for 111 data set.\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/Distortion_of_hierarchical_and_k-means_for_111.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Comparison between Distortion of hierarchical and k-means methods for 290 data set.\n",
    "\n",
    "<img src =\"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/Distortion_of_hierarchical_and_k-means_for_290.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. Comparison between Distortion of hierarchical and k-means methods for 896 data set.\n",
    "\n",
    "<img src =\"https://raw.githubusercontent.com/akgarhwal/Coursera-Fundamentals-of-Computing/master/Algorithmic%20Thinking%20-Part%202/Week-1%20and%20Week-2/Comparison%20of%20Clustering%20Algorithms/Distortion_of_hierarchical_and_k-means_for_896.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. Analysis of Distortion by Hierarchical and k-means for above 3 set.\n",
    "\n",
    "Ans : For the 111 county data set, hierarchical clustering consistently produces clusterings with less distortion. <br>\n",
    "For the other two data sets, neither clustering method consistently dominates. \n",
    "\n",
    "For our knowledge : <br>\n",
    "Interestingly, k-means clustering produces lower distortion clusterings for the 3108 county data set. <br>\n",
    "<a href=\"http://storage.googleapis.com/codeskulptor-alg/matplotlib_distortion_3018.png\">Link</a> to a plot of distortion for the clusterings produced by both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. Which clustering method would you prefer when analyzing these data sets?\n",
    "\n",
    "On these data sets, neither method dominates in all three areas: efficiency, automation, and quality. In terms of efficiency, k-means clustering is preferable to hierarchical clustering as long as the desired number of output clusters is known beforehand. However, in terms of automation, k-means clustering suffers from the drawback that a reliable method for determining the initial cluster centers needs to be available. Finally, in terms of quality, neither method produces clusterings with consistently lower distortion on larger data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For More details please visit to <a href=\"https://github.com/akgarhwal/Coursera-Fundamentals-of-Caomputing\">Abhinesh Garhwal @Github </a>\n",
    "\n",
    "<a href = \"https://github.com/akgarhwal/Coursera-Fundamentals-of-Computing.git\">GitHub Repository</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
